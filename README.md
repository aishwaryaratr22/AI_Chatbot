# AI_Chatbot
A simple chatbot app built with Streamlit, powered by Ollama for local LLM inference and LangChain for conversational logic.

Local LLM Inference: Uses Ollama to run large language models (LLMs) locally, ensuring privacy and offline capability.

Conversational Memory: Maintains context across interactions using LangChain memory modules.

Streamlit UI: Clean and interactive chat interface built with Streamlit for quick deployment and ease of use.

Custom System Prompts: Ability to define and modify system instructions to guide chatbot behavior.

Model Flexibility: Easily switch between different LLMs supported by Ollama (e.g., LLaMA, Mistral, Gemma).

Modular Architecture: Designed with reusable and extendable components for easier customization and scaling.

Fast Setup: Minimal configuration required—just clone, install dependencies, and run.

Lightweight & Secure: All processing is done locally—no data sent to external servers.







